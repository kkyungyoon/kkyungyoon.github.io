---
title: "Boostcamp AI Tech_Week01_Day01"
date: 2024-08-05
layout: post
tags: [Naver Boostcamp, daily report]
---

# 강의 내용 복습
***
## 1. [PyTorch] 01. PyTorch 기초

### Pytorch 아키텍처  
1) Top-level API  
두 번째 레벨인 engine level과 파이썬 API를 통해 연결됨  
2) Mid-level Engine  
3) Low-level Library  
사용자의 텐서연산 담당, gpu를 담당하는 cuda등 실제 하드웨어와 연동해 연산 수행  
<br><br>

### Pytorch 사용하면 좋은 점  
1) 간편한 사용성  
2) 강력한 커뮤니티와 생태계  
3) 동적 계산 그래프  
4) 대규모 병렬 연산을 효율적으로 수행할 수 있음  
<br><br>

### Tensor 생성 코드  
 1) 0차원 텐서 :  
 ```
 a = torch.tensor(argument=scalar)
 ```  
 2) 1차원 텐서 : 

 ```  
 a = torch.tensor([1,2,3,4,5])
 ```  

 - dim-0에 집중(차원이 늘어날 때마다 축의 위치가 변경됨)  
 
 3) 2차원 텐서 :  
 ```
 a = torch.tensor([[1,2,3],[4,5,6],[7,8,9]])
 ```  
 - dim-0이 새로운 행차원 축이 되고, 기존축은 dim-1이 됨  
 - 칸안의 값들은 gray scale에서의 밝기 정도  
 
 4) 3차원 텐서 : 2차원 텐서들을 포함하는 텐서  
 - 축 회전을 시켜서 각각의 요소별로 채널의 값을 합해서 색을 출력함  
 
 5) 4차원 텐서 : 시간이라는 축이 생김(영상생성 가능)  
<br><br>

## 2. [PyTorch] 02. Data type & Basic Functions

###  Pytorch 데이터 타입  

#### 1. 정수형 데이터 타입  

1) 8비트 부호없는 정수    
```
dtype = torch.uint8
```
2) 8비트 부호있는 정수  
```
dtype = torch.int8
```
3) 16비트 부호있는 정수  
```
dtype = torch.int16
dtype = torch.short
```
4) 32비트 부호있는 정수  
```
dtype = torch.int32
dtype = torch.int
```
5) 64비트 부호있는 정수  
```
dtype = torch.int64
dtype = torch.long
```
<br><br>

#### 2. 실수형 데이터 타입  
신경망 수치에 사용되기 때문에 가장 중요한 데이터 타입  

1) 고정 소수점 수  
- 16비트 고정 소수점 수  
- 컴퓨터가 메모리를 감당하기 어려운 문제점 발생   
<br>

2) 부동 소수점 수  
- 가수부와 지수부로 나누는 정규화 진행  
- 32비트 부동 소수점 수  

```
dtype = torch.float32
dtype = torch.float
```  

- 64비트 부동 소수점 수  

```
dtype = torch.float64
dtype = torch.double
```  

<br><br>

### 타입 캐스팅

#### 1. 32비트 부동 소수점 수로 변환  
```
j = 변환하고자하는 tensor.float()
```
#### 2. 64비트 부동 소수점 수로 변환  
```
j = 변환하고자하는 tensor.double()
```
<br><br>

### Tensor 기초함수

#### 1. Tensor의 요소를 반환하건 계산하는 함수
- torch.max(tensor)  
- torch.min(tensor)  
- torch.prod(tensor) : 모두 곱한 값
- torch.mean(tensor)  
- torch.var(tensor) : 표본분산
- torch.std(tensor)  
<br><br>

#### 2. Tensor의 특성을 확인하는 메서드 
- 차원의 수

```
tensor_name.dim()
```   

- 크기, 모양 확인  

```
tensor_name.size()
tensor_name.shape
```    

- 요소의 총 개수

```
tensor_name.numel()  
```

<br><br>

## 3. [PyTorch] 03. Creating Tensors

### Tensor의 생성

#### 1. 특정한 값으로 초기화된 Tensor 생성 
1) 0으로 초기화된 Tensor를 생성하는 표현  
```
a = torch.zeros(3)

# dim-0 (새로운 행차원), dim-1(기존 열차원)
b = torch.zeros([2, 3])

# dim-0 (새로운 depth차원), dim-1(행), dim-2(열)
c = torch.zeros([3, 2, 4])

# 사이즈 확인
print(c.size())

# 타입 확인
print(c.dtype)
```
<br><br>

2) 1으로 초기화된 Tensor를 생성하는 표현  
```
a = torch.ones(3)

# dim-0 (새로운 행차원), dim-1(기존 열차원)
b = torch.ones([2, 3])

# dim-0 (새로운 depth차원), dim-1(행), dim-2(열)
c = torch.ones([3, 2, 4])
```
<br><br>

#### 2. 특정한 값으로 초기화된 Tensor 변환  
1) 크기와 자료형이 같은 0으로 초기화된 Tensor로 변환하는 표현  
```
# 중요한 포인트 : 메모리 주소가 변하지않음
g = torch.zeros_like(e)
```
2) 크기와 자료형이 같은 1로 초기화된 Tensor로 변환하는 표현  
```
g = torch.ones_like(e)
```
<br><br>

#### 3. 난수로 초기화된 Tensor 생성  
1) [0, 1] 구간의 연속균등분포 난수 Tensor 생성
```
i = torch.rand(3) #요소가 3개인 1차원 텐서 생성
j = torch.rand([2, 3])
```

2) 표준정규분포 난수 Tensor 생성  
```
k = torch.randn(3)
k = torch.randn(3, dtype= torch.int) # 에러발생
# RuntimeError: "normal_kernel_cpu" not implemented for 'Int'
l = torch.randn([2, 3])
```

3) 크기와 자료형이 같은 [0, 1] 구간의 연속균등분포 난수 Tensor로 변환
```
torch.rand_like(k)
```

4) 크기와 자료형이 같은 표준정규분포 난수 Tensor로 변환
```
torch.randn_like(i)
```
<br><br>

#### 4. 지정된 범위 내에서 초기화된 Tensor 생성  

* 지정된 범위 내에서 일정 간격의 값을 가진 Tensor를 생성하는 표현  

```
a = torch.arange(start, end, step)
# step은 소수 간격으로도 가능 -> 데이터타입이 부동소수점 수로 나옴
```
<br><br>

#### 5. 초기화되지 않은 Tensor 생성  
```
# 초기화되지 않은 Tensor 생성
q = torch.empty(5)

# 그 텐서를 다른 데이터로 수정(메모리 주소는 그대로)
q.fill_(3.0)

torch.fill_(q, 3.0)
```
<br><br>

#### 6. list, Numpy 데이터로부터 Tensor 생성  
```
# list 생성 후, 텐서 생성하는 코드
t = torch.tensor(list)

# 넘파이로부터 Tensor 생성
v = torch.from_numpy(u).float()
# 넘파이로부터 생성된 텐서는 정수형이므로 실수형으로 타입 캐스팅
```
<br><br>  

#### 7. CPU Tensor 생성  

1) 정수형 CPU Tensor 생성하는 표현
```
w = torch.IntTensor([1,2,3,4,5])
torch.ByteTensor
torch.CharTensor
torch.ShortTensor
torch.LongTensor
```

2) 실수형 CPU Tensor 생성하는 표현
```
A, B, C, D, E = 1, 2, 3, 4, 5
x= torch.FloatTensor([A, B, C, D, E])
torch.DoubleTensor
```
<br><br>  

#### 8. Tensor의 복제  
```
y = x.clone()
z = x.detach()
# x.detach는 x를 계산 그래프에서 분리하여 새로운 텐서 z에 저장한다는 것
# 자동미분때 다시 등장
```
<br><br>

#### 9. CUDA Tensor 생성과 변환  
```
# 디바이스 확인
tensor_name.device

# cuda를 사용할 수 있는 환경인지 확인
torch.cuda.is_available()

# cuda 디바이스 이름 확인
torch.cuda.get_device_name(device=0)

# 사용가능한 GPU 개수 확인
torch.cuda.device_count()

# tensor를 gpu에 할당

b = torch.tensor([1,2,3,4,5]).to('cuda')
b = torch.tensor([1,2,3,4,5]).cuda()

# GPU에 할당된 Tensor를 CPU로
c = b.to('cpu')
c = b.cpu()
```

<br><br>

***

# 피어세션 정리
- 그라운드 룰 정하기
- 줌 이론멘토링 시간 정하고 멘토님께 말씀드리기
- PyTorch 강의, 퀴즈, 과제 계획 세우기
- 정규과정 외에 추가공부 정하기
- 자기소개 ppt 내용 상의

***
<br><br>

# 학습회고

엄청 들어오고 싶었던 네이버 부스트캠프 AI Tech 교육이 시작했다.  
팀원분들도 새로 만나고, 여러모로 설렜던 시간이었다.  
앞으로 7개월동안 악착같이 과정 따라가서 엄청 성장해있고싶다.

<br><br>