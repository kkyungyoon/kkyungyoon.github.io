---
title: "Boostcamp AI Tech_Week01_Day02"
date: 2024-08-06
layout: post
tags: [Naver Boostcamp, daily report]
---

## 강의 내용 복습

***

### 1. [PyTorch] 04. Manipulation of Tensors 

#### 1) 텐서도 인덱싱, 슬라이싱이 가능하다.  
```
# 1-D tensor  
# indexing : a[1]
# slicing : a[1:4]

# 2-D tensor   
# indexing : b[1,2]
# slicing : b[0,:], b[0, ...]
```

<br>

#### 2) Tensor의 모양 변경에 이용되는 함수를 배웠다.  

```
 ########################## view() 메소드 #############################
 - 메모리가 연속적으로 할당된 경우에만 사용가능   
 - 비연속적 메모리할당 ex, 슬라이싱   
 - contiguous 속성이 깨졌다고 함   
 - 비연속적인지, 연속적인지 확인해야함  
 
 c.is_contiguous()
 # True : 연속
 # False : 비연속 -> c.contiguous()를 통해 연속적으로 만들어줄 수 있음
 
 f.view(4,-1)
 f.view(depth, row, column)
 
 ########################## flatten() 함수 ############################
 - 실제로 정말 많이 사용함  
 = torch.flatten(b)
 = b.flatten()
 
 # 만약 b의 shape이  3, 2, 2 라면  
 = torch.flatten(b, 0) #0부터 마지막 차원까지 평탄화 = 3*2*2로 이해함  
 = torch.flatten(b, 1) #1부터 마지막 차원까지 평탄화 = 3, (2*2)로 이해함   
 
 ######################### reshape() 메소드  ###########################
 - 장점 : 유연성좋음(메모리가 연속적이지 않아도 사용 가능), 안전함  
 - 단점 : 성능저하발생가능성(아예 새로운 복사본을 할당하는거라)  
 = b.reshape(3,4)
 
 ######################### transpose() 메소드  #########################
 - 축을 바꿈  
 = b.transpose(1, 2)

 ######################### squeeze() 함수  ############################
 - dim이 1인 특정 차원의 축소  

 a = torch.randn(1,2,3)
 = torch.squeeze(a) # 2행 3열 2차원 텐서

 a = torch.randn(1,1,3)
 = torch.squeeze(a) # 3개의 요소로 이뤄진 1차원 텐서

 a = torch.randn(1,1,3)
 = torch.squeeze(a, dim=0) # depth 축소

 a = torch.randn(1,1,3)
 = torch.squeeze(a, dim=1) # row 축소
 

 ######################### unsqueeze() 함수를 활용한 차원확장 ############## 
 - dim이 1인 특정 차원의 확장    
 - 머신러닝에서 자주 쓰이는 함수    
 a = torch.randn(2,3)
 = torch.unsqueeze(a, dim=0) # shape : 1,2,3
 = torch.unsqueeze(a, dim=1) # shape : 2,1,3
 = torch.unsqueeze(a, dim=2) # shape : 2,3,1

 ######################### stack() 함수를 활용한 결합  ####################
 dimension 0인 축을 생성해서 결합  
 = torch.stack((red_channel, green_channel, blue_channel))
 dimension 1인 축을 생성해서 결합
 = torch.stack((red_channel, green_channel, blue_channel), dim=1)
 ```

<br><br>


### 2. [PyTorch] 05. Basic Operations  

#### 1) 모양변경하는 함수를 배웠다.    

```
######################### cat() ############################
- stack() : 새로운 차원 생성    
- cat() : 기존차원유지 + 연결    
= torch.cat((b, c)) # 행방향
= torch.cat((b, c), 1) # 열방향
- 단, 붙일 때 크기가 맞아야함 -> 안 맞으면, reshape 진행  

######################### expand() ######################### 
- expand 활용하려면, 행 or 열 중에 하나가 1이어야함  
= b.expand(3,3)

######################### repeat() ######################### 
- 장점 : expand의 한계 개선    
- 단점 : expand보다 메모리 효율성 떨어짐(왜냐, 새로운 메모리공간 할당됨)  
= b.repeat(2, 3) #dim-0으로 2번, dim-1으로 3번
```

<br>

#### 2) 산술연산  

- 더하기  

 ```
 torch.add(a,b)

 # 메모리 주소 다름
 print(id(a))
 print(id(b))

 # in-place 방식
 a.add_(b)
 # 장점 : 메모리 절약 + 텐서 값 업데이트
 # 단점 : autograd와의 호환성 측면 문제 일으킬 수 있음
 # id(a.add_(b)) = a의 메모리 주소와 같음

 # 크기가 다른 더하기
 torch.add(a,b) # 브로드캐스팅
 ```

 - 빼기  

 ```
 torch.sub(a, b)

 # in-place 방식
 a.sub_(b)

 # 크기가 다른 빼기
 torch.sub(a, b) # 브로드캐스팅
 ```

 - 스칼라곱  

 ```
 torch.mul(scalar, tensor)
 ```

 - 요소별 곱하기  

 ```
 torch.mul(a, b)

 # in-place 방식
 a.mul_(b)

 # 크기가 다른 요소별 곱하기
 torch.mul(a, b)
 ```

 - 요소별 나누기  

 ```
 torch.div(a, b)

 # in-place 방식
 a.div_(b) -> in-place 방식으로 했는데, 에러뜨면, a, b 데이터타입을 부동소수점으로 바꿨는지 체크(a=a.float())

 # 크기가 다른 요소별 나누기
 torch.div(a, b)
 ```

 - 요소별 거듭제곱  

 ```
 torch.pow(a, n)
 torch.pow(a, b)

 # in-place 방식
 a.pow_(b)
 ```

 - 요소별 거듭제곱근  

 ```
 torch.pow(a, 1/n)
 ```

 <br>

#### 2) 비교연산  

```
torch.eq(a, b) # 같은지  
torch.ne(a, b) # 다른지
torch.gt(a, b) # 큰지
torch.ge(a, b) # 크거나 같은지
torch.lt(a, b) # 작은지
torch.le(a, b) # 작거나 같은지
```

<br>

#### 3) 논리연산  

```
# 논리곱(AND)연산
torch.logical_and(a, b)

# 논리합(OR)연산
torch.logical_or(a, b)

# 배타적논리합(XOR)연산
# 하나만 참일 때
torch.logical_xor(a, b)
```

<br><br>


### 3. [PyTorch] 06. Tensor Operations  

#### 1) Tensor의 노름이 텐서를 비교하는 방법으로 쓰인다는 점을 배웠다.   

#### 2) Ln 노름  

```
# L1 norm   
- 절댓값의 합, 맨해튼 노름, 마름모  
torch.norm(a, p=1)

# L2 norm  
- 제곱합의 제곱근, 원, 유클리드 노름  
torch.norm(a, p=2)

# L-infinity norm  
- 절댓값 중 최댓값, 정사각형  
torch.norm(a, p=float('inf'))
```

#### 3) 유사도  
- 클러스터링 알고리즘에서 중요한 기준으로 사용됨   

```
# 맨해튼 거리
dist = torch.norm(b-c, p=1)
# 맨해튼 유사도
1/(1 + dist)
```

```
# 유클리드 거리
dist = torch.norm(b-c, p=2)
# 유클리드 유사도
1/(1 + dist)
```

```
내적 구하는 법  
# 방법 1
torch.dot(a,b)

# 방법 2
torch.dot(a,b) = (torch.norm(a, p=2)*torch.norm(b, p=2)) * 코사인유사도
``` 

```
# 코사인 유사도 
torch.dot(a,b) / (torch.norm(a, p=2)*torch.norm(b, p=2))
```

<br>

#### 4) 2-D Tensor 행렬연산   

```
# 행렬곱 
A.matmul(B)
A.mm(B)
A@B
```

- 이용해서 이미지 좌우, 위아래 뒤집기 생각해볼 수 있음  
<br><br>

***


## 피어세션 정리  
- '피어세션이 피어씁니다' ppt 검토  
- 서로 궁금했던 질문들  
- '피어세션이 피어씁니다' 예행연습  
<br>

***

## 과제수행과정  
첫번째 과제 : PyTorch에서 Tensor를 다루는 다양한 방법들을 경험

***

## 학습회고  
텐서를 다루는 여러 방법을 배우고, 헷갈렸던 개념들을 정립할 수 있었다.
<br><br>