---
title: "Boostcamp AI Tech_Week01_Day02"
date: 2024-08-06
layout: post
tags: [Naver Boostcamp, daily report]
---

## 강의 내용 복습

***

### 1. [PyTorch] 04. Manipulation of Tensors

#### 1-D tensor   

```
# indexing
a[1]

# slicing
a[1:4]
``` 

<br>

#### 2-D tensor   

```
# indexing
b[1,2]

# slicing
b[0,:], b[0, ...]
```

<br>

#### Tensor의 모양 변경   

 1) view() 메소드    
 - Tensor의 메모리가 연속적으로 할당된 경우에만 사용가능함   
 - [x] 비연속적 메모리할당 ex, 슬라이싱   
 - [x] contiguous 속성이 깨졌다 고 함   
 - 비연속적인지, 연속적인지 확인해야함  

 ```
 c.is_contiguous()
 # True : 연속
 # False : 비연속 -> c.contiguous()를 통해 연속적으로 만들어줄 수 있음
 ```

 - 텐서 모양변경  

 ```
 f.view(4,-1)
 f.view(depth, row, column)
 ```

 2) flatten() 함수    
 - 실제로 정말 많이 사용함  
 
 ```  
 a = torch.flatten(b)
 a = b.flatten()
 
 # 만약 b의 shape이  3, 2, 2 라면  
 a = torch.flatten(b, 0) #0부터 마지막 차원까지 평탄화 = 3*2*2로 이해함  
 a = torch.flatten(b, 1) #1부터 마지막 차원까지 평탄화 = 3, (2*2)로 이해함  
 ```  
 
 3) reshape() 메소드   
 - 장점 : 유연성좋음(메모리가 연속적이지 않아도 사용 가능), 안전함  
 - 단점 : 성능저하발생가능성(아예 새로운 복사본을 할당하는거라)  

 ```
 a = b.reshape(3,4)
 ```

 4) transpose() 메소드  
 - 축을 바꿈  

 ```
 a = b.transpose(1, 2)
 ```

 5) squeeze() 함수를 활용한 차원축소  
 - dim이 1인 특정 차원의 축소  

 ```
 a = torch.randn(1,2,3)
 b = torch.squeeze(a) # b : 2행 3열 2차원 텐서

 a = torch.randn(1,1,3)
 b = torch.squeeze(a) # b : 3개의 요소로 이뤄진 1차원 텐서

 a = torch.randn(1,1,3)
 b = torch.squeeze(a, dim=0) # b : depth 축소

 a = torch.randn(1,1,3)
 b = torch.squeeze(a, dim=1) # b : row 축소
 ```

 6) unsqueeze() 함수를 활용한 차원확장  
 - dim이 1인 특정 차원의 확장    
 - 머신러닝에서 자주 쓰이는 함수    

 ```
 a = torch.randn(2,3)
 b = torch.unsqueeze(a, dim=0) # b의 shape : 1,2,3
 b = torch.unsqueeze(a, dim=1) # b의 shape : 2,1,3
 b = torch.unsqueeze(a, dim=2) # b의 shape : 2,3,1
 ```

 7) stack() 함수를 활용한 결합  

 ```
 # dimension 0인 축을 생성해서 결합  
 a = torch.stack((red_channel, green_channel, blue_channel))
 
 # dimension 1인 축을 생성해서 결합
 a = torch.stack((red_channel, green_channel, blue_channel), dim=1)
 ```

<br><br>


### 2. [PyTorch] 05. Basic Operations  

#### 모양변경  

1) cat()  
- stack() : 새로운 차원 생성    
- cat() : 기존차원유지 + 연결    

```
a = torch.cat((b, c)) # 행방향
a = torch.cat((b, c), 1) # 열방향
```

- 단, 붙일 때 크기가 맞아야함 -> 안 맞으면, torch.cat(b, c.reshape(e,f)) 진행  

2) expand()  
- expand 활용하려면, 행 or 열 중에 하나가 1이어야함  

```
a = b.expand(3,3)
```

3) repeat()  
- 장점 : expand의 한계 개선    
- 단점 : expand보다 메모리 효율성 떨어짐(왜냐, 새로운 메모리공간 할당됨)  

```
a = b.repeat(2, 3) #dim-0으로 2번, dim-1으로 3번
```

<br><br>

#### 기초연산  

1) 산술연산  

- 더하기  

 ```
 torch.add(a,b)

 # 메모리 주소 다름
 print(id(a))
 print(id(b))

 # in-place 방식
 a.add_(b)
 # 장점 : 메모리 절약 + 텐서 값 업데이트
 # 단점 : autograd와의 호환성 측면 문제 일으킬 수 있음
 # id(a.add_(b)) = a의 메모리 주소와 같음

 # 크기가 다른 더하기
 torch.add(a,b) # 브로드캐스팅
 ```

 - 빼기  

 ```
 torch.sub(a, b)

 # in-place 방식
 a.sub_(b)

 # 크기가 다른 빼기
 torch.sub(a, b) # 브로드캐스팅
 ```

 - 스칼라곱  

 ```
 torch.mul(scalar, tensor)
 ```

 - 요소별 곱하기  

 ```
 torch.mul(a, b)

 # in-place 방식
 a.mul_(b)

 # 크기가 다른 요소별 곱하기
 torch.mul(a, b)
 ```

 - 요소별 나누기  

 ```
 torch.div(a, b)

 # in-place 방식
 a.div_(b) -> in-place 방식으로 했는데, 에러뜨면, a, b 데이터타입을 부동소수점으로 바꿨는지 체크(a=a.float())

 # 크기가 다른 요소별 나누기
 torch.div(a, b)
 ```

 - 요소별 거듭제곱  

 ```
 torch.pow(a, n)
 torch.pow(a, b)

 # in-place 방식
 a.pow_(b)
 ```

 - 요소별 거듭제곱근  

 ```
 torch.pow(a, 1/n)
 ```

 <br><br>

2) 비교연산  

```
torch.eq(a, b) # 같은지  
torch.ne(a, b) # 다른지
torch.gt(a, b) # 큰지
torch.ge(a, b) # 크거나 같은지
torch.lt(a, b) # 작은지
torch.le(a, b) # 작거나 같은지
```

<br><br>

3) 논리연산  

```
# 논리곱(AND)연산
torch.logical_and(a, b)

# 논리합(OR)연산
torch.logical_or(a, b)

# 배타적논리합(XOR)연산
# 하나만 참일 때
torch.logical_xor(a, b)
```

<br><br>


### 3. [PyTorch] 06. Tensor Operations  

#### Tensor의 노름  
- 텐서를 비교하는 방법  

#### Ln 노름  
1) L1 norm   
- 절댓값의 합  
- 맨해튼 노름  
- 마름모  

```
torch.norm(a, p=1)
```

2) L2 norm  
- 제곱합의 제곱근  
- 원  
- 유클리드 노름  

```
torch.norm(a, p=2)
```

3) L-infinity norm  
- 절댓값 중 최댓값  
- 정사각형  

```
torch.norm(a, p=float('inf'))
```
<br><br>

#### 유사도  
- 클러스터링 알고리즘에서 중요한 기준으로 사용됨  

1) 맨해튼 유사도  
- 맨해튼 거리가 작을수록, 맨해튼 유사도는 커짐  
- 1에 가까울수록 유사함  

```
# 맨해튼 거리
manhattan_distance = torch.norm(b-c, p=1)
# 맨해튼 유사도
1/(1 + manhattan_distance)
```

2) 유클리드 유사도  
- 유클리드 거리가 작을수록, 유클리드 유사도는 커짐  
- 1에 가까울수록 유사함  

```
# 유클리드 거리
euclidean_distance = torch.norm(b-c, p=2)
# 유클리드 유사도
1/(1 + euclidean_distance)
```

3) 내적 구하는 법  

```
# 방법 1
torch.dot(a,b)

# 방법 2
torch.dot(a,b) = (torch.norm(a, p=2)*torch.norm(b, p=2)) * 코사인유사도
```

4) 코사인 유사도  

```
torch.dot(a,b) / (torch.norm(a, p=2)*torch.norm(b, p=2))
```

<br><br>

#### 2-D Tensor 행렬연산   

```
# 행렬곱 
A.matmul(B)
A.mm(B)
A@B
```

- 이용해서 이미지 좌우, 위아래 뒤집기 생각해볼 수 있음  
<br><br>

***


## 피어세션 정리  
- '피어세션이 피어씁니다' ppt 검토  
- 서로 궁금했던 질문들  
- '피어세션이 피어씁니다' 예행연습  
<br><br>

***

## 과제수행과정  
첫번째 과제였는데, 일단 아직까지 배운 torch 기능들을 숙달해서 쓰지는 못 하고,  
공부한 거를 그때그때 찾아가보면서 문제를 푼 거 같다.  
그래도 확실한거는 과제를 풀면서 계속 쓰다보니 조금씩 익숙해지는 것 같다.  
일단 풀기는 다 풀었고, 오늘 학습정리 끝나면 지금 복습하러 갈 예정이다.  

***

## 학습회고  
나한테 이렇게 오롯이 공부만 할 수 있는 시간이 또 있을까 싶고,  
오늘 멘토링 시간 때, 비전공자 신분으로 공부하는 것이 좀 막막해 멘토님께 질문을 드렸는데,  
버티면서 꾸준히 계속 해야한다는 말을 해주셨다.  
내가 정말 잘 하고 싶다고 생각하게 된 학문이니만큼 꼭 버텨서 전문가가 되고싶다.  
내일도 화이팅~ 기죽지말자~  
<br><br>