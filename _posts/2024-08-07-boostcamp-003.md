---
title: "Boostcamp AI Tech_Week01_Day03"
date: 2024-08-07
layout: post
tags: [Naver Boostcamp, daily report]
---

## 강의 내용 복습

***

### 1. [PyTorch] 07. Linear Regression

#### 캐글 데이터 셋 다운로드, 압축 해제   

```
!kaggle datasets download -d 업로드한 사람 이름/데이터 셋 이름
!unzip 파일이름.zip
``` 

<br>

#### 데이터 할당 

```
x = data.iloc[:,:-1].values
y = data.iloc[:,-1].values
```

<br>

#### 상관관계 분석   

1) 선형관계 파악 가능  
2) 양의 관계 / 음의 관계 파악 가능  
3) 높은 상관관계 가지는 변수들 알 수 있음 (다중 선형 회귀에서 유용)

```
np.corrcoef(x, y)

# 시각화
plt.scatter(x, y)
```

#### 텐서로, 넘파이로 바꾸기

```
# 텐서로
torch.tensor()

# 넘파이로
.values
```

<br><br>

#### nn.Module
nn.Module을 상속받아 Linear regression 클래스 정의

```
class 클래스이름(nn.Module):
```

<br><br>

#### 클래스, 인스턴스, 메서드, 속성  
클래스 : 메서드, 속성을 정의함   
인스턴스 : 클래스에 생성된 구체적인 객체  
메서드 : 클래스 내부 함수  
속성 :  클래스 내부 변수  

```
# 인스턴스
model = LinearRegressionModel()

# 생성자 메서드
def __init__(self):
    super(LinearRegressionModel.self).__init__()

# 순전파 메서드
def forward(self, x):
    y = self.linear(x)

# 속성
self.linear = nn.Linear(1, 1)
```

<br><br>

#### 손실함수
손실함수 = loss function = 비용함수 = cost function

```
loss_function = nn.MSELoss()
```

<br><br>

### 2. [PyTorch] 08. Linear Regression  

#### 경사하강법 = 배치 경사하강법
손실함수 값이 최소가 되도록 하는 가중치 w와 bias b 찾는 방법  
전체 데이터 셋을 사용해 최적화함

1) 학습률  
- 가중치가 업데이트되는 크기를 결정  
- 시행착오가 답이다  
- 학습률 너무 커도 안 되고, 너무 작아도 안 됨

2) 가중치 w 값 업데이트

```
optimizer.step()
```

3) 이전 단계 기울기 초기화

```
optimizer.zero_grad()
```

<br><br>

#### 확률적 경사하강법(stochastic gradient descent)
배치 경사하강법의 한계(로컬미니마) 극복  
각 데이터 포인트마다 오차를 계산(= 시그마 기호 사라짐)

```
import torch.optim as optim
optimizer = optim.SGD(model.parameters(), lr=-0.01)
optimizer.zero_grad()
loss.backward()
optimizer.step()
``` 

<br><br>

#### 에폭  
모델이 전체 데이터셋을 한 번 쫙 훑는 과정

- 너무 과하면, 과적합  
- 너무 적으면, 학습이 제대로 안 됨  

```
num_epochs = 1000
for epoch in range(epochs):
    y = model(x_tensor)
    loss = loss_function(y, t_tensor)
```

<br><br>

#### 데이터 표준화

1) 손실값이 큰 경우  
- 학습률을 조정해보기
- 데이터를 시각화해보기
- 에폭수 조정해보기  
- 데이터 전처리 (중 하나가 표준화)

2) 

```
from sklearn.preprocessing import StandardScaler
scaler_x = StandardScaler()
scaled_x = scaler_x.fit_transform(x.reshape(-1, 1)) # 2차원 배열 필요
```
<br><br>


***


## 피어세션 정리  
- 오늘 다른 조의 피어세션 그라운드 룰을 보고, 추가하면 좋을 것 같은 내용을 건의
- 공부하며 궁금했던 거 서로 물어보는 시간  
- 정규 진도를 다 나간 이후에 뭘 하면 좋을지 논의(코테공부, 유튜브 강의 듣기 등)  


***

## 과제수행과정  
두 번째 과제였는데, 첫 번째 과제보다 양은 적었지만 과제를 풀었다고 끝이 아니라  
다시 복습하고 또 복습할수록 내가 무심결에 넘어간 코드가 있다는 걸 알았다.  
개인노션에다가 코드 한 블록씩 직접 따라 쓰면서 복습 중이다. 

***

## 학습회고  
오늘로 3일차! 하루하루 알차게 보내자!  
나는 팀원 분들이 좋은데, 팀원 분들은 나를 어떻게 생각하실지 궁금하다  
<br><br>