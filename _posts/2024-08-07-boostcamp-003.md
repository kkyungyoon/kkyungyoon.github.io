---
title: "Boostcamp AI Tech_Week01_Day03"
date: 2024-08-07
layout: post
tags: [Naver Boostcamp, daily report]
---

## 강의 내용 복습

***

### 1. [PyTorch] 07. Linear Regression

#### 1) 상관관계 분석 내용과 코드를 배웠다.  

```
np.corrcoef(x, y)
plt.scatter(x, y) # 시각화
```

#### 2) 텐서로, 넘파이로 바꾸는 코드를 배웠다.  

```
# 텐서로
torch.tensor()

# 넘파이로
.values
```

#### 3) 클래스, 인스턴스, 메서드, 속성을 확실히 구분한다.  
클래스 : 메서드, 속성을 정의함   
인스턴스 : 클래스에 생성된 구체적인 객체  
메서드 : 클래스 내부 함수  
속성 :  클래스 내부 변수  

```
# 인스턴스
model = LinearRegressionModel()

# 생성자 메서드
def __init__(self):
    super(LinearRegressionModel.self).__init__()

# 순전파 메서드
def forward(self, x):
    y = self.linear(x)

# 속성
self.linear = nn.Linear(1, 1)
```

#### 4) 손실함수의 개념과 구하는 코드를 배웠다.  
손실함수 = loss function = 비용함수 = cost function

```
= nn.MSELoss()
```

<br><br>

### 2. [PyTorch] 08. Linear Regression  

#### 1) 경사하강법 = 배치 경사하강법 같은 말이고, 손실함수 값이 최소가 되도록 하는 가중치 w와 bias b 찾는 방법이라는 걸 배웠다.    
특징은 전체 데이터 셋을 사용해 최적화한다는 것이다.  

```
optimizer.step()
optimizer.zero_grad()
```


#### 2) 확률적 경사하강법(stochastic gradient descent). 흔히 SGD 라고 봤던 기법이었다.  
배치 경사하강법의 한계(로컬미니마) 극복하고, 각 데이터 포인트마다 오차를 계산하기 때문에 시그마 기호 사라진다는 걸 배웠다. 

```
optimizer = optim.SGD(model.parameters(), lr=-0.01)
optimizer.zero_grad()
loss.backward()
optimizer.step()
``` 


#### 3) 에폭이란, 모델이 전체 데이터셋을 한 번 쫙 훑는 과정이고, 
- 너무 과하면, 과적합  
- 너무 적으면, 학습이 제대로 안 됨  

```
for epoch in range(epochs):
    y = model(x_)
    loss = loss_function(y, t)
```



#### 4) 손실값이 큰 경우 학습률을 조정해보거나, 데이터를 시각화해보거나, 에폭수를 조정해보거나, 데이터 전처리 (중 하나가 표준화)를 하는 등의 방법이 있는 것을 배웠다.  

```
from sklearn.preprocessing import StandardScaler
scaler_x = StandardScaler()
scaled_x = scaler_x.fit_transform(x.reshape(-1, 1)) # 2차원 배열 필요
```
<br><br>


***


## 피어세션 정리  
- 오늘 다른 조의 피어세션 그라운드 룰을 보고, 추가하면 좋을 것 같은 내용을 건의
- 공부하며 궁금했던 거 서로 물어보는 시간  
- 정규 진도를 다 나간 이후에 뭘 하면 좋을지 논의(코테공부, 유튜브 강의 듣기 등)  


***

## 과제수행과정  
두 번째 과제
- nn.Module을 상속받아 클래스를 정의하는 과정을 직접 해봄
- MSE 객체를 생성해 다중선형회귀모형을 학습해봄

***

## 학습회고  
과제가 아닌 부분도 한 줄씩 코드를 직접 손으로 따라쳐봐야겠다
<br><br>