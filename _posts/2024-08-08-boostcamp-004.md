---
title: "Boostcamp AI Tech_Week01_Day04"
date: 2024-08-08
layout: post
tags: [Naver Boostcamp, daily report]
---

## 강의 내용 복습

### 1. [PyTorch] 09. Binary Classification

#### 선형회귀 모델 테스트

```
# 테스트 데이터 표준화
= scaler_x.transform(test.reshape(-1, 1))

# 텐서로 변환, 필요한 디바이스로 이동
= torch.tensor(스케일된 데이터, dtype=torch.float32).view(-1, 1).to(device)
``` 

1) 배열의 차원 1차원이냐 2차원이냐 그때그때 잘 신경써야 할 거 같다  
2) device가 어디냐도 잘 신경써야 할 거 같다

```
model.eval() # 평가모드
with.torch.no_grad() # 메모리사용 줄일 수 있음
= model(test_tensor) # 예측
= scaler_t.inverse_transform(모델에 test_tensor넣어서 나온 값.cpu().numpy())
"""
원래 스케일로 돌림
넘파이 배열로 변환하려면, gpu에 있는 걸 cpu로 다시 가져와야함
표준화 해제하는 작업을 해줘야함
"""
```

<br>

####  이진 분류 모델

```
# 원하는 열만 뽑아오기
df = pd.read_csv('파일명', sep=',', header=0)[[열1, 열2]]

# 그 열에서 원하는 값만 뽑아오기
filtered_data = df[df[열이름].isin([카테고리1, 카테고리2])]

# 포함되면 true, 아니면 false인 boolean series
isin([카테고리1, 카테고리2])

# 범주형 목표변수 -> 이산형 레이블로 매핑
= filtered_data[열이름].map({카테고리1:0, 카테고리2:1})

# 특징변수 
데이터프레임(2차원) -> 넘파이배열(2차원)
= filtered_data[[특징변수에 해당하는 열]].values

# 목표변수
Series형식(1차원) -> 넘파이배열(1차원)
= filtered_data[열이름].values.astype(int)

# 데이터 분할
from sklearn.model_selection import train_test_split
train_test_split(특징변수, 목표변수, test_size=0.2, random_state=42)

# 데이터 표준화
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test) 

# 텐서로 변환, 차원 추가(배치처리때문)
= torch.tensor(x_train, dtype=float32).unsqueeze(1)
```

<br>

#### Dataset & DataLoader 클래스   

```
# 미니배치 경사하강법 
- 경사하강법 단점 : 로컬미니마, 계산비용문제
- 확률적 경사하강법 단점 : 노이즈많음, 학습과정 불안정
- 미니배치 경사하강법 : 두 단점 보완
from torch.utils.data import DataLoader, TensorDataset

# Dataset 상속 -> 클래스 정의
- 상속받았으니 __init__, __len__, __getitem__ 함수 필요    
Class 클래스이름(Dataset):
- __init__ 함수 : 인스턴스 생성될 때 호출되는 생성자 메서드  
- __len__ 함수 : 데이터 셋 크기 반환  
- __getitem__ 함수 : 인덱스에 해당하는 특징변수, 목표변수 리턴해줌

# Dataset, DataLoader 클래스 
Dataset을 상속받아 만든 클래스로, 만든 인스턴스를 감싸서, 배치단위로 데이터 로드 = DataLoader  
- 훈련 : shuffle 함 (순서없애려고)  
- 평가 : shuffle 안함
total_train = IrisDataset(x_train, t_train)
total_test = IrisDataset(x_test, t_test)
train_loader = DataLoader(total_train, batch_size=, shuffle=True)
test_loader = DataLoader(total_test, batch_size=, shuffle=False)
```
<br><br>

#### 이진분류모델 - 로지스틱회귀알고리즘

```
# 순서 : 최적의 결정경계 찾기  -> 시그모이드함수를 통해 분류 (시그모이드 함수 식 꼭 기억)  
# 장점 : 결과를 확률로 해석가능
class 클래스이름(nn.Module):

# 생성자 메서드 __init__   
- super(지금 클래스이름, self).__init__ : 부모클래스의 생성자 메서드를 호출해서 상속받고 초기화 수행  
- 선형계층정의  
- 시그모이드함수정의  

# 순전파 메서드 forward  
- __init__에서 정의한 layer에 객체 x 넣어서 z 받기  
- __init__에서 정의한 sigmoid 객체에 z 넣어서 y 받기
# 이렇게 만들어진 클래스로 인스턴스 모델 생성
```
<br><br>


### 2. [PyTorch] 10. Binary Classification

#### 이진 교차 엔트로피  

```
# 선형회귀에서는 손실함수로 MSE를 사용했는데, 여기서는 이진교차엔트로피(BCE) 사용함  
- return으로 받은 y는 0-1 사이 값 가지는 확률  
- 확률변수 t는 0 또는 1  
- 1, 2번 합쳐서 베르누이분포 pdf를 만들 수 있음  
- 가능도함수로 바라본 상태에서, 계산을 위해서 log 씌워 product를 시그마로 바꿈
loss_funtion = nn.BCELoss()
```
<br>


#### 궁금해서 찾아본 내용 : [torch.optim](https://pytorch.org/docs/stable/optim.html){: target="_blank"}

1) 종류   
Adadelta, Adagrad, Adam, AdamW, SparseAdam, Adamax, ASGD, LBFGS, NAdam, RAdam, RMSprop, Rprop, SGD

2) optimizer 생성
```
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
optimizer = optim.Adam([var1, var2], lr=0.0001)

# to specify per layer learning rates
optim.SGD([
                {'params': model.base.parameters(), 'lr': 1e-2},
                {'params': model.classifier.parameters()}
            ], lr=1e-3, momentum=0.9)
```


3) taking an optimization step  

```
for input, target in dataset:
    optimizer.zero_grad() # Resets the gradients of all optimized torch.Tensor
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step() # Performs a single optimization step (parameter update)
```
<br>

#### 궁금해서 찾아본 내용 : [torch.nn](https://pytorch.org/docs/stable/nn.html){: target="_blank"}

1) torch.nn -> Containers -> Module, Sequential, ModuelList, ModuelDict, ParameterList, ParameterDict  

2) torch.nn.Module : Base class for all neural network modules.  

3) Modules can also contain other Modules  

```
import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
```
<br>

#### 이진 분류 모델의 테스트
```
1) 테스트  
model.eval() # 평가모드
with torch.no_grad(): # 기울기없이계산
= (predictions > 0.5).float() 
# int로 변환해도 돌아가길래 궁금해서 팀원분께 여쭤보니, 실제 y값이 실수일수도 있어서 비교할 때 에러뜰까봐 float으로 한거같다고 말씀해주셨다

2) 예측, 실제 라벨 출력
= t_test.numpy() # 텐서 -> 넘파이 (시각화 및 비교하기 위해서)
print(predict_label.flatten())
print(actual_label.flatten())
```
<br><br>

***

## 피어세션 정리  
- 피어세션 때 질문 다 하고 시간 남으면 코테 풀기로 해서, 문제 1개 풀어봤다.  
- 코딩테스트 준비할 책을 하나 골랐다.  

***

## 과제수행과정  
세 번째 과제
- 로지스틱 회귀 분석에 필요한 pytorch 구성 요소들을 구현
- 로지스틱 회귀 모델을 구현

***

## 학습회고  
복습도 굉장히 중요할 것 같다는 생각이다.
<br><br>